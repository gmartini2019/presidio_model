{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from typing import List, Optional, Dict, Union, Iterator, Iterable\n",
    "import collections\n",
    "from dataclasses import dataclass\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import EngineResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_analyzer = AnalyzerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/mnt/c/Users/marti/Downloads/final_testing.csv')\n",
    "testing_ds = pd.read_csv('/mnt/c/Users/marti/Downloads/sample_2.csv')\n",
    "testing_ds_second = pd.read_csv('/mnt/c/Users/marti/Downloads/sample3.csv')\n",
    "testing_ds_third = pd.read_csv('/mnt/c/Users/marti/Downloads/sample4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "from io import StringIO\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def dataset_analysis(dataset, analyzer=None, cutoff=0.6):\n",
    "   \n",
    "    # create analyzer engine\n",
    "    if analyzer == None:\n",
    "        analyzer = global_analyzer\n",
    "\n",
    "    # create a dictionary to store the findings for each column\n",
    "    all_columns_info = {}\n",
    "\n",
    "    # analyze each value and record entity scores and counts\n",
    "    for col in dataset.columns:\n",
    "        # create dictionaries to store entity scores and counts for each column\n",
    "        entity_scores = defaultdict(list)\n",
    "        entity_counts = defaultdict(int)\n",
    "\n",
    "        total_rows = len(dataset[col])\n",
    "\n",
    "        for value in dataset[col]:\n",
    "            # Call analyzer to get results\n",
    "            try:\n",
    "                results = analyzer.analyze(text=value, language='en')\n",
    "                \n",
    "                if not results:  # If no entities are found\n",
    "                    entity_counts['NOT PII'] += 1\n",
    "\n",
    "                # iterate over each result\n",
    "                for result in results:\n",
    "                    # add score and count to entity_scores and entity_counts dictionaries\n",
    "                    entity_scores[result.entity_type].append(result.score)\n",
    "                    entity_counts[result.entity_type] += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        # calculate the total entities found in the column\n",
    "        total_entities = sum(entity_counts.values())\n",
    "\n",
    "        # calculate average score, count, and percentage for each entity\n",
    "        entity_scores_counts = {}\n",
    "        for entity, scores in entity_scores.items():\n",
    "            score_avg = sum(scores) / len(scores)\n",
    "            \n",
    "            if score_avg < cutoff:\n",
    "                entity_counts['NOT PII'] += entity_counts.pop(entity)\n",
    "            else:\n",
    "                count = entity_counts[entity]\n",
    "                percentage = (count / total_rows) * 100\n",
    "                entity_scores_counts[entity] = {'score_average': score_avg, 'number_of_datapoints': count, 'percentage': percentage}\n",
    "\n",
    "        # Add the 'NOT PII' class and its percentage\n",
    "        not_pii_count = entity_counts['NOT PII']\n",
    "        not_pii_percentage = (not_pii_count / total_rows) * 100\n",
    "        entity_scores_counts['NOT PII'] = {'score_average': 0, 'number_of_datapoints': not_pii_count, 'percentage': not_pii_percentage}\n",
    "\n",
    "        # store findings for the current column\n",
    "        all_columns_info[col] = entity_scores_counts\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict({(i, j): all_columns_info[i][j]\n",
    "                                        for i in all_columns_info.keys()\n",
    "                                        for j in all_columns_info[i].keys()},\n",
    "                                        orient='index')\n",
    "\n",
    "    results_df.columns = ['score_average', 'n_datapoints', 'percentage']\n",
    "\n",
    "    # display the results\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "@app.post(\"/analyze-csv\")\n",
    "async def analyze_csv(file: UploadFile = File(...)):\n",
    "    # Read the uploaded CSV file\n",
    "    content = await file.read()\n",
    "    content_str = content.decode()\n",
    "    dataset = pd.read_csv(StringIO(content_str))\n",
    "\n",
    "    # Perform the analysis\n",
    "    results_df = dataset_analysis(dataset)\n",
    "\n",
    "    # Convert the results DataFrame to a dictionary and return it\n",
    "    results_dict = results_df.reset_index().to_dict(orient=\"records\")\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def take_time(dataset, analyzer = None):\n",
    "    row_count = dataset.shape[0]\n",
    "    start = time.time() \n",
    "    dataset_analysis(dataset, global_analyzer)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'For {row_count} and {dataset.shape[1]} columns, it took {end - start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            score_average  n_datapoints  percentage\n",
      "address      ADDRESS                 0.70           297        59.4\n",
      "             LOCATION                0.85            37         7.4\n",
      "             PERSON                  0.85            17         3.4\n",
      "             DATE_TIME               0.85            11         2.2\n",
      "             NRP                     0.85             4         0.8\n",
      "             NOT PII                 0.00           172        34.4\n",
      "currency     NOT PII                 0.00           500       100.0\n",
      "postalZip    DATE_TIME               0.85            56        11.2\n",
      "             PERSON                  0.85             4         0.8\n",
      "             NOT PII                 0.00           462        92.4\n",
      "phone        DATE_TIME               0.85             3         0.6\n",
      "             UK_NHS                  1.00            18         3.6\n",
      "             NOT PII                 0.00           487        97.4\n",
      "name         PERSON                  0.85           456        91.2\n",
      "             LOCATION                0.85             7         1.4\n",
      "             NOT PII                 0.00            40         8.0\n",
      "country      LOCATION                0.85           491        98.2\n",
      "             NOT PII                 0.00             9         1.8\n",
      "region       LOCATION                0.85           298        59.6\n",
      "             NRP                     0.85            14         2.8\n",
      "             PERSON                  0.85            90        18.0\n",
      "             NOT PII                 0.00           110        22.0\n",
      "email        EMAIL_ADDRESS           1.00           411        82.2\n",
      "             PERSON                  0.85            13         2.6\n",
      "             NRP                     0.85             4         0.8\n",
      "             LOCATION                0.85             2         0.4\n",
      "             NOT PII                 0.00           800       160.0\n",
      "list         NOT PII                 0.00             0         0.0\n",
      "alphanumeric PERSON                  0.85            11         2.2\n",
      "             LOCATION                0.85             4         0.8\n",
      "             NRP                     0.85             6         1.2\n",
      "             NOT PII                 0.00           479        95.8\n",
      "numberrange  NOT PII                 0.00             0         0.0\n",
      "For 500 and 11 columns, it took 24.55925726890564 seconds\n"
     ]
    }
   ],
   "source": [
    "take_time(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   score_average  n_datapoints  percentage\n",
      "name     PERSON             0.85            99        99.0\n",
      "         LOCATION           0.85             1         1.0\n",
      "         NOT PII            0.00             1         1.0\n",
      " age     NOT PII            0.00             0         0.0\n",
      " gender  NOT PII            0.00           100       100.0\n",
      " country LOCATION           0.85             8         8.0\n",
      "         PERSON             0.85             4         4.0\n",
      "         NOT PII            0.00            87        87.0\n",
      "For 100 and 4 columns, it took 1.4725275039672852 seconds\n"
     ]
    }
   ],
   "source": [
    "take_time(testing_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            score_average  n_datapoints  percentage\n",
      "country      LOCATION                0.85           943        94.3\n",
      "             NRP                     0.85            37         3.7\n",
      "             PERSON                  0.85            35         3.5\n",
      "             NOT PII                 0.00            55         5.5\n",
      "first_name   PERSON                  0.85           530        53.0\n",
      "             LOCATION                0.85            49         4.9\n",
      "             NRP                     0.85            13         1.3\n",
      "             DATE_TIME               0.85             1         0.1\n",
      "             NOT PII                 0.00           407        40.7\n",
      "last_name    PERSON                  0.85           451        45.1\n",
      "             LOCATION                0.85            46         4.6\n",
      "             NRP                     0.85            12         1.2\n",
      "             NOT PII                 0.00           491        49.1\n",
      "email        EMAIL_ADDRESS           1.00          1000       100.0\n",
      "             PERSON                  0.85            43         4.3\n",
      "             NRP                     0.85            23         2.3\n",
      "             DATE_TIME               0.85             2         0.2\n",
      "             LOCATION                0.85             3         0.3\n",
      "             NOT PII                 0.00          1636       163.6\n",
      "email2       EMAIL_ADDRESS           1.00          1000       100.0\n",
      "             PERSON                  0.85            40         4.0\n",
      "             NRP                     0.85            18         1.8\n",
      "             LOCATION                0.85             4         0.4\n",
      "             NOT PII                 0.00          1636       163.6\n",
      "profession   NOT PII                 0.00          1000       100.0\n",
      "city         LOCATION                0.85           723        72.3\n",
      "             PERSON                  0.85           127        12.7\n",
      "             NRP                     0.85             3         0.3\n",
      "             NOT PII                 0.00           151        15.1\n",
      "country_code LOCATION                0.85            93         9.3\n",
      "             NRP                     0.85             3         0.3\n",
      "             PERSON                  0.85            22         2.2\n",
      "             NOT PII                 0.00           877        87.7\n",
      "For 1000 and 8 columns, it took 40.935956716537476 seconds\n"
     ]
    }
   ],
   "source": [
    "take_time(testing_ds_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        score_average  n_datapoints  percentage\n",
      "date   DATE_TIME                 0.85          1000       100.0\n",
      "       NOT PII                   0.00             0         0.0\n",
      "lorem  PERSON                    0.85            44         4.4\n",
      "       LOCATION                  0.85            38         3.8\n",
      "       DATE_TIME                 0.85             4         0.4\n",
      "       NRP                       0.85             3         0.3\n",
      "       NOT PII                   0.00           911        91.1\n",
      "random NOT PII                   0.00             0         0.0\n",
      "guid   DATE_TIME                 0.85            55         5.5\n",
      "       PERSON                    0.85            45         4.5\n",
      "       NRP                       0.85            10         1.0\n",
      "       LOCATION                  0.85             7         0.7\n",
      "       MEDICAL_LICENSE           1.00             3         0.3\n",
      "       NOT PII                   0.00           906        90.6\n",
      "For 1000 and 4 columns, it took 18.869446516036987 seconds\n"
     ]
    }
   ],
   "source": [
    "take_time(testing_ds_third)\n",
    "full_ds = pd.read_csv('/mnt/c/Users/marti/Downloads/final_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_column(df):\n",
    "    # List the columns with numbers\n",
    "    print(\"Columns:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i}: {col}\")\n",
    "\n",
    "    # Get the user input for the column number\n",
    "    col_num = int(input(\"Enter the column number you want to select: \"))\n",
    "\n",
    "    # Check if the input is a valid column number\n",
    "    if 0 <= col_num < len(df.columns):\n",
    "        # Return the selected column\n",
    "        selected_column = df[[df.columns[col_num]]]\n",
    "        print(f\"Selected column '{df.columns[col_num]}':\")\n",
    "        print(selected_column)\n",
    "        return selected_column\n",
    "    else:\n",
    "        print(\"Invalid column number. Exiting.\")\n",
    "        return\n",
    "def user_column_detection(df, analyzer = global_analyzer):\n",
    "    selected_column = get_csv_column(df)\n",
    "    dataset_analysis(selected_column, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "0: address\n",
      "1: currency\n",
      "2: postalZip\n",
      "3: phone\n",
      "4: name\n",
      "5: country\n",
      "6: region\n",
      "7: email\n",
      "8: list\n",
      "9: alphanumeric\n",
      "10: numberrange\n",
      "Selected column 'address':\n",
      "                           address\n",
      "0               9503 Curabitur Rd.\n",
      "1    P.O. Box 143, 2253 Aenean Rd.\n",
      "2                 914-6334 Sed Av.\n",
      "3          613-4380 Iaculis Avenue\n",
      "4              739-678 Lectus. Rd.\n",
      "..                             ...\n",
      "495           Ap #974-697 Elit St.\n",
      "496                290-4464 In Rd.\n",
      "497                7136 Massa. St.\n",
      "498         2335 Pellentesque, Rd.\n",
      "499                  1708 Diam Ave\n",
      "\n",
      "[500 rows x 1 columns]\n",
      "                   score_average  n_datapoints  percentage\n",
      "address ADDRESS             0.70           297        59.4\n",
      "        LOCATION            0.85            37         7.4\n",
      "        PERSON              0.85            17         3.4\n",
      "        DATE_TIME           0.85            11         2.2\n",
      "        NRP                 0.85             4         0.8\n",
      "        NOT PII             0.00           172        34.4\n"
     ]
    }
   ],
   "source": [
    "user_column_detection(dataset, global_analyzer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER customizable modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: PERSON, start: 0, end: 7, score: 0.85, type: ADDRESS, start: 22, end: 33, score: 0.7]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from presidio_analyzer import Pattern, PatternRecognizer\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "\n",
    "# Rule based model\n",
    "class AddressRecognizer(PatternRecognizer):\n",
    "    PATTERNS = [\n",
    "        Pattern(\"Address (Simple Regex)\", r\"\\d{1,5}\\s\\w+\\s(?:St|Ave|Ln|Dr|Rd|Blvd)\", 0.7),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, patterns: List[Pattern] = None, context: str = None):\n",
    "        super().__init__(\n",
    "            supported_entity=\"ADDRESS\",\n",
    "            patterns=patterns if patterns else self.PATTERNS,\n",
    "            context=context,\n",
    "            supported_language=\"en\",\n",
    "        )\n",
    "\n",
    "address_recognizer = AddressRecognizer()\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "\n",
    "custom_registry = RecognizerRegistry(recognizers=registry.recognizers)\n",
    "custom_registry.add_recognizer(address_recognizer)\n",
    "registry.add_recognizer(address_recognizer)\n",
    "global_analyzer = AnalyzerEngine(registry=custom_registry)\n",
    "\n",
    "text = \"Roberto lives in Five 10 Broad St.\"\n",
    "numbers_results = global_analyzer.analyze(text=text, language=\"en\")\n",
    "print(numbers_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "0: address\n",
      "1: currency\n",
      "2: postalZip\n",
      "3: phone\n",
      "4: name\n",
      "5: country\n",
      "6: region\n",
      "7: email\n",
      "8: list\n",
      "9: alphanumeric\n",
      "10: numberrange\n",
      "Selected column 'address':\n",
      "                           address\n",
      "0               9503 Curabitur Rd.\n",
      "1    P.O. Box 143, 2253 Aenean Rd.\n",
      "2                 914-6334 Sed Av.\n",
      "3          613-4380 Iaculis Avenue\n",
      "4              739-678 Lectus. Rd.\n",
      "..                             ...\n",
      "495           Ap #974-697 Elit St.\n",
      "496                290-4464 In Rd.\n",
      "497                7136 Massa. St.\n",
      "498         2335 Pellentesque, Rd.\n",
      "499                  1708 Diam Ave\n",
      "\n",
      "[500 rows x 1 columns]\n",
      "                   score_average  n_datapoints  percentage\n",
      "address ADDRESS             0.70           297        59.4\n",
      "        LOCATION            0.85            37         7.4\n",
      "        PERSON              0.85            17         3.4\n",
      "        DATE_TIME           0.85            11         2.2\n",
      "        NRP                 0.85             4         0.8\n",
      "        NOT PII             0.00           172        34.4\n"
     ]
    }
   ],
   "source": [
    "user_column_detection(dataset, global_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: CITY, start: 37, end: 41, score: 1.0, type: PERSON, start: 0, end: 7, score: 0.85, type: LOCATION, start: 37, end: 41, score: 0.85, type: ADDRESS, start: 22, end: 33, score: 0.7]\n"
     ]
    }
   ],
   "source": [
    "city_list = list(testing_ds_second['city'])\n",
    "city_recognizer = PatternRecognizer(supported_entity=\"CITY\", deny_list=city_list)\n",
    "custom_registry.add_recognizer(city_recognizer)\n",
    "global_analyzer = AnalyzerEngine(registry=custom_registry)\n",
    "text = \"Roberto lives in Five 10 Broad St in Ipoh\"\n",
    "numbers_results = global_analyzer.analyze(text=text, language=\"en\")\n",
    "print(numbers_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "0: country\n",
      "1: first_name\n",
      "2: last_name\n",
      "3: email\n",
      "4: email2\n",
      "5: profession\n",
      "6: city\n",
      "7: country_code\n",
      "Selected column 'country':\n",
      "            country\n",
      "0             Haiti\n",
      "1           Eritrea\n",
      "2             Samoa\n",
      "3           AndorrA\n",
      "4           Ireland\n",
      "..              ...\n",
      "995       Singapore\n",
      "996     Isle of Man\n",
      "997       Lithuania\n",
      "998      Kyrgyzstan\n",
      "999  United Kingdom\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "                 score_average  n_datapoints  percentage\n",
      "country CITY               1.0            28         2.8\n",
      "        NOT PII            0.0           972        97.2\n"
     ]
    }
   ],
   "source": [
    "user_column_detection(testing_ds_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_747/314519889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"address\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ADDRESS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for address in dataset[\"address\"]:\n",
    "    train_data.append((address, {\"entities\": [(0, len(address), \"ADDRESS\")]}))\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Load the base model\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.add_pipe(\"ner\", source=spacy.load(\"en_core_web_sm\"))\n",
    "\n",
    "# Add the new entity label to the NER model\n",
    "ner.add_label(\"ADDRESS\")\n",
    "\n",
    "# Initialize the weights of the model\n",
    "examples = [Example.from_dict(nlp(text), annotations) for text, annotations in train_data]\n",
    "nlp.initialize(lambda: examples)\n",
    "\n",
    "# Train the NER model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    losses = {}\n",
    "    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, losses=losses)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {losses['ner']}\")\n",
    "\n",
    "# Save the trained model\n",
    "#nlp.to_disk(\"my_custom_ner_model\")\n",
    "\n",
    "#nlp = spacy.load(\"my_custom_ner_model\")\n",
    "\n",
    "# Test the model on some text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fbc4efc7c10>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: CITY, start: 0, end: 4, score: 1.0, type: CITY, start: 5, end: 12, score: 1.0, type: LOCATION, start: 0, end: 4, score: 0.85, type: LOCATION, start: 5, end: 12, score: 0.85]\n"
     ]
    }
   ],
   "source": [
    "text = 'Ipoh Atlanta'\n",
    "numbers_results = global_analyzer.analyze(text=text, language=\"en\")\n",
    "print(numbers_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'process_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_696/127305548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Roberto lives in Five 10 Broad St.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnumbers_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/presidio_analyzer/analyzer_engine.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text, language, entities, correlation_id, score_threshold, return_decision_process, ad_hoc_recognizers, context, allow_list, nlp_artifacts)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# a NlpArtifacts instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnlp_artifacts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mnlp_artifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_decision_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'process_text'"
     ]
    }
   ],
   "source": [
    "analyzer_2 = AnalyzerEngine(\n",
    "    nlp_engine=nlp, \n",
    "    supported_languages=[\"en\"]\n",
    ")\n",
    "text = \"Roberto lives in Five 10 Broad St.\"\n",
    "numbers_results = analyzer_2.analyze(text=text, language=\"en\")\n",
    "print(numbers_results)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruce_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
